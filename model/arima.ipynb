{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Danh s√°ch file CSV: ['AHT.csv', 'BARC.csv', 'BATS.csv', 'BP.csv', 'HLMA.csv', 'JMAT.csv', 'LGEN.csv', 'PSON.csv', 'REL.csv', 'RKT.csv', 'SDR.csv', 'SGE.csv', 'SHEL.csv', 'SSE.csv', 'STAN.csv', 'STJ.csv', 'TSCO.csv', 'VOD.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu\n",
    "folder_path = \"../Data_FTSE_model\"\n",
    "\n",
    "# L·∫•y danh s√°ch t·∫•t c·∫£ file CSV trong th∆∞ m·ª•c\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "\n",
    "# Ki·ªÉm tra danh s√°ch file\n",
    "print(\"üìÇ Danh s√°ch file CSV:\", csv_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tham s·ªë c·ª≠a s·ªï tr∆∞·ª£t\n",
    "train_size = 750\n",
    "test_size = 250\n",
    "step_size = 250  # M·ªói l·∫ßn d·ªãch c·ª≠a s·ªï\n",
    "\n",
    "# Duy·ªát t·ª´ng file ƒë·ªÉ hu·∫•n luy·ªán ARIMA v√† d·ª± b√°o\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path, parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "\n",
    "    # T·ªïng s·ªë ƒëi·ªÉm d·ªØ li·ªáu\n",
    "    total_points = len(df)\n",
    "\n",
    "    # L∆∞u k·∫øt qu·∫£ d·ª± b√°o\n",
    "    all_forecasts = []\n",
    "\n",
    "    # C·ª≠a s·ªï tr∆∞·ª£t\n",
    "    for start in range(0, total_points - train_size - test_size + 1, step_size):\n",
    "        train_end = start + train_size\n",
    "        test_end = train_end + test_size\n",
    "\n",
    "        # T·∫°o t·∫≠p train v√† test\n",
    "        train_data = df.iloc[start:train_end][\"Close\"]\n",
    "        test_dates = df.index[train_end:test_end]\n",
    "\n",
    "        try:\n",
    "            # Hu·∫•n luy·ªán m√¥ h√¨nh ARIMA(p=1, d=1, q=1)\n",
    "            model = ARIMA(train_data, order=(1,1,1))\n",
    "            model_fit = model.fit()\n",
    "\n",
    "            # D·ª± b√°o 250 ng√†y ti·∫øp theo\n",
    "            forecast = model_fit.forecast(steps=test_size)\n",
    "            forecast.index = test_dates\n",
    "\n",
    "            # L∆∞u k·∫øt qu·∫£ v√†o danh s√°ch\n",
    "            forecast_df = pd.DataFrame({\"Date\": test_dates, \"Predicted_Close\": forecast.values})\n",
    "            all_forecasts.append(forecast_df)\n",
    "\n",
    "            print(f\"‚úÖ Ho√†n th√†nh c·ª≠a s·ªï {start} ‚Üí {test_end} ({file})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è L·ªói v·ªõi {file} (t·ª´ {start}): {e}\")\n",
    "\n",
    "    # G·ªôp t·∫•t c·∫£ d·ª± b√°o l·∫°i v√† l∆∞u file\n",
    "    if all_forecasts:\n",
    "        final_forecast_df = pd.concat(all_forecasts)\n",
    "        output_file = '../output/arima/ARIMA_' + file\n",
    "        final_forecast_df.to_csv(output_file, index=False)\n",
    "        print(f\"üìÅ D·ª± b√°o ho√†n th√†nh: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
