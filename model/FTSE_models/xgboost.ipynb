{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/TSLA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path, features_importance=None):\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Lag Features\n",
    "    data['lag_1'] = data['Close'].shift(1)\n",
    "    data['lag_5'] = data['Close'].shift(5)\n",
    "    data['lag_10'] = data['Close'].shift(10)\n",
    "\n",
    "    # Rolling Mean Features\n",
    "    data['rolling_mean_5'] = data['Close'].rolling(window=5).mean()\n",
    "    data['rolling_mean_10'] = data['Close'].rolling(window=10).mean()\n",
    "    data['rolling_mean_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "    # Rolling Std Features\n",
    "    data['rolling_std_5'] = data['Close'].rolling(window=5).std()\n",
    "    data['rolling_std_10'] = data['Close'].rolling(window=10).std()\n",
    "    data['rolling_std_20'] = data['Close'].rolling(window=20).std()\n",
    "\n",
    "    # Expanding Mean Feature\n",
    "    data['expanding_mean'] = data['Close'].expanding().mean()\n",
    "\n",
    "    # Xử lý giá trị thiếu và vô hạn\n",
    "    \n",
    "    data = data.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "\n",
    "    data.drop(columns=['High', 'Low', 'Open', 'Volume'], inplace=True)\n",
    "    data['ParabolicSAR'] = data['ParabolicSAR'].shift(1)\n",
    "    data['TrueRange'] = data['TrueRange'].shift(1)\n",
    "    \n",
    "    if features_importance is not None:\n",
    "        features_importance.append('Close')\n",
    "        features_importance.append('Date')\n",
    "        data = data[features_importance]\n",
    "\n",
    "    data.fillna(method='bfill', inplace=True)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(stock_data, sequence_length=10, target_column='Close', train_size=325, test_size=125, step=125):\n",
    "    \"\"\"\n",
    "    Chuẩn bị dữ liệu cho mô hình LSTM và XGBoost, bao gồm chuẩn hóa và dịch chuyển tập huấn luyện, kiểm tra.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stock_data : pandas DataFrame\n",
    "        Dữ liệu cổ phiếu đã được xử lý\n",
    "    sequence_length : int\n",
    "        Độ dài chuỗi thời gian cho LSTM\n",
    "    target_column : str\n",
    "        Tên cột dữ liệu mục tiêu cần dự đoán\n",
    "    train_size : int\n",
    "        Số lượng mẫu dành cho tập huấn luyện\n",
    "    test_size : int\n",
    "        Số lượng mẫu dành cho tập kiểm tra\n",
    "    step : int\n",
    "        Số lượng mẫu dịch chuyển giữa các lần huấn luyện\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List chứa các bộ dữ liệu huấn luyện và kiểm tra đã xử lý cho LSTM và XGBoost\n",
    "    \"\"\"\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "    stock_data = stock_data.sort_values('Date').reset_index(drop=True)\n",
    "    datasets = []\n",
    "    \n",
    "    for start_idx in range(0, len(stock_data) - (train_size + test_size), step):\n",
    "        df_train = stock_data.iloc[start_idx:start_idx + train_size]\n",
    "        df_test = stock_data.iloc[start_idx + train_size - sequence_length:start_idx + train_size + test_size]\n",
    "        \n",
    "        feature_columns = [col for col in stock_data.columns if col not in ['Date', target_column]]\n",
    "        \n",
    "        X_train = df_train[feature_columns].values\n",
    "        X_test = df_test[feature_columns].values\n",
    "        y_train = df_train[target_column].values\n",
    "        y_test = df_test[target_column].values\n",
    "        \n",
    "        # Chuẩn hóa dữ liệu\n",
    "        scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "        \n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        X_test_scaled = scaler_X.transform(X_test)\n",
    "        \n",
    "        y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
    "        y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))\n",
    "        \n",
    "        # Chuẩn bị dữ liệu cho XGBoost\n",
    "        X_train_xgb = X_train_scaled\n",
    "        X_test_xgb = X_test_scaled\n",
    "        y_train_xgb = y_train_scaled.flatten()\n",
    "        y_test_xgb = y_test_scaled.flatten()\n",
    "        \n",
    "        # Chuẩn bị dữ liệu cho LSTM\n",
    "        X_sequences, y_sequences = [], []\n",
    "        for i in range(len(X_train_scaled) - sequence_length):\n",
    "            X_sequences.append(X_train_scaled[i:i + sequence_length])\n",
    "            y_sequences.append(y_train_scaled[i + sequence_length])\n",
    "        \n",
    "        X_sequences = np.array(X_sequences)\n",
    "        y_sequences = np.array(y_sequences)\n",
    "        \n",
    "        X_test_sequences, y_test_sequences = [], []\n",
    "        for i in range(len(X_test_scaled) - sequence_length):\n",
    "            X_test_sequences.append(X_test_scaled[i:i + sequence_length])\n",
    "            y_test_sequences.append(y_test_scaled[i + sequence_length])\n",
    "        \n",
    "        X_test_sequences = np.array(X_test_sequences)\n",
    "        y_test_sequences = np.array(y_test_sequences)\n",
    "        \n",
    "        datasets.append({\n",
    "            'xgboost': {\n",
    "                'X_train': X_train_xgb, \n",
    "                'y_train': y_train_xgb,\n",
    "                'X_test': X_test_xgb[sequence_length:], \n",
    "                'y_test': y_test_xgb[sequence_length:],\n",
    "                'feature_names': feature_columns,\n",
    "                'scaler_X': scaler_X,\n",
    "                'scaler_y': scaler_y\n",
    "            },\n",
    "            'lstm': {\n",
    "                'X_train': X_sequences, \n",
    "                'y_train': y_sequences,\n",
    "                'X_test': X_test_sequences, \n",
    "                'y_test': y_test_sequences,\n",
    "                'scaler_y': scaler_y,\n",
    "                'scaler_X': scaler_X,\n",
    "                'sequence_length': sequence_length\n",
    "            },\n",
    "            'dates_test': df_test['Date'].values[sequence_length:],\n",
    "            'actual_test': y_test[sequence_length:]\n",
    "        })\n",
    "    \n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xgboost_model(prepared_data, max_depth=7, learning_rate=0.1, n_estimators=100, early_stopping_rounds=10):\n",
    "    \"\"\"\n",
    "    Xây dựng và huấn luyện mô hình XGBoost với dữ liệu đã chuẩn hóa\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prepared_data : dict\n",
    "        Dictionary chứa dữ liệu đã chuẩn bị cho XGBoost\n",
    "    max_depth : int\n",
    "        Độ sâu tối đa của cây\n",
    "    learning_rate : float\n",
    "        Tốc độ học của mô hình\n",
    "    n_estimators : int\n",
    "        Số lượng cây ước lượng\n",
    "    early_stopping_rounds : int\n",
    "        Số vòng đợi trước khi early stopping\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Mô hình XGBoost đã huấn luyện và kết quả dự đoán\n",
    "    \"\"\"\n",
    "    # Lấy dữ liệu\n",
    "    X_train = prepared_data['xgboost']['X_train']\n",
    "    y_train = prepared_data['xgboost']['y_train']\n",
    "    X_test = prepared_data['xgboost']['X_test']\n",
    "    feature_names = prepared_data['xgboost']['feature_names']\n",
    "    scaler_y = prepared_data['xgboost']['scaler_y']\n",
    "    \n",
    "    X_train_xgb, X_val_xgb, y_train_xgb, y_val_xgb = train_test_split(\n",
    "        X_train, y_train, test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train_xgb, y_train_xgb,\n",
    "        eval_set=[(X_val_xgb, y_val_xgb)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    y_pred_scaled = model.predict(X_test)\n",
    "\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_importance_dict = {feature: importance for feature, importance in zip(feature_names, feature_importance)}\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'predictions_scaled': y_pred_scaled,\n",
    "        'feature_importance': feature_importance_dict\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
